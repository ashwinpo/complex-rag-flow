{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5fd940-d1ab-4be2-9193-295ae6f0f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --quiet duckduckgo_search==5.3.1b1\n",
    "!pip install -U --quiet ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aaed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import re\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Watson Discovery\n",
    "from ibm_watson import DiscoveryV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# Watsonx\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Langchain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fa7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "credentials = Credentials(\n",
    "                   url = os.getenv('WATSONX_URL'),\n",
    "                   api_key = os.getenv('WATSONX_APIKEY'),\n",
    "                  )\n",
    "try:\n",
    "    project_id = os.getenv(\"WATSONX_INSTANCE_ID\")\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "    \n",
    "\n",
    "authenticator = IAMAuthenticator(os.getenv('WATSONX_APIKEY'))\n",
    "discovery_project_id = os.getenv(\"DISCOVERY_PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99f159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"meta-llama/llama-3-70b-instruct\"\n",
    "\n",
    "# Greedy model for most prompts\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "}\n",
    "\n",
    "\n",
    "watsonx_llama3 = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6785dc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Capitalism is a complex and multifaceted economic system that has both positive and negative impacts on society. While it has lifted millions of people out of poverty, a significant portion of the population remains in poverty, and income inequality has increased. Capitalism promotes innovation, efficiency, and economic growth, but it also creates social and environmental problems, such as exploitation of workers, environmental degradation, and concentration of wealth. Ultimately, whether capitalism is good or bad for society depends on how it is regulated and implemented, and whether its benefits are shared equitably among all members of society.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test if the model is reasonable:\n",
    "watsonx_llama3.invoke('Argue if capitalism is good or bad for society. Summarize your conclusions in a concise sentences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'snippet': \"Krishna talked with CNBC about his specific views on regulation, the business of generative AI and IBM's successes and mistakes. IBM CEO Arvind Krishna speaks at a panel session at the World ...\", 'title': 'IBM CEO Arvind Krishna CNBC interview', 'link': 'https://www.cnbc.com/2023/12/07/ibm-ceo-arvind-krishna-cnbc-interview.html'}, {'snippet': 'Lou Gerstner (born March 1, 1942, Mineola, New York, U.S.) is an American businessman best known for the pivotal role he played in revitalizing the ailing IBM in the mid-1990s; he served as CEO of the company from 1993 to 2002. Gerstner studied engineering at Dartmouth College in Hanover, New Hampshire (B.A., 1963), where he graduated magna cum ...', 'title': 'Lou Gerstner | Biography, IBM, & Facts | Britannica Money', 'link': 'https://www.britannica.com/money/Lou-Gerstner'}, {'snippet': 'IBM did see momentum in its AI business, however. The company revealed it had more than $1 billion in AI-related backlog. Krishna says companies are coming to IBM to unlock new productivity, with ...', 'title': 'IBM CEO Arvind Krishna: Why the economy has slowed, and how AI is ...', 'link': 'https://finance.yahoo.com/news/ibm-ceo-arvind-krishna-why-the-economy-has-slowed-and-how-ai-is-benefiting-220239991.html'}, {'snippet': 'Arvind Krishna has been serving as IBM CEO since 2020. Indian American Arvind Krishna is the Chairman and Chief Executive Officer of technology giant IBM. Born in India, Krishna earned his ...', 'title': 'Arvind Krishna: Chairman and CEO of IBM - The American Bazaar', 'link': 'https://www.americanbazaaronline.com/2024/03/17/arvind-krishna-bio-456260/'}]\n"
     ]
    }
   ],
   "source": [
    "### Search - DuckDuckGo\n",
    "\n",
    "web_search_tool = DuckDuckGoSearchResults(max_results=5)\n",
    "\n",
    "docs = web_search_tool.invoke(\"CEO of IBM?\")\n",
    "\n",
    "def parse_ddg_results(input_string:str) -> List[str]:\n",
    "    # Regex to match each entry in the string\n",
    "    pattern = r'\\[snippet: (.*?), title: (.*?), link: (.*?)\\]'\n",
    "    # Find all matches using regex\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    # Create a list of dictionaries based on the matches\n",
    "    data = [{'snippet': match[0], 'title': match[1], 'link': match[2]} for match in matches]\n",
    "    return data\n",
    "\n",
    "# Output the result\n",
    "print(parse_ddg_results(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d44f5",
   "metadata": {},
   "source": [
    "## Discovery Related Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063b6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_collection_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are helpful assistant writing a short summary so a user know's what's included. \n",
    "    Summarize the folowing retrieved context. Limit your summary to 3 sentences. Don't include extraneous information! \n",
    "    Only respond with the summary with no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context} \n",
    "    Summary: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "summary_chain = summarize_collection_prompt | watsonx_llama3 | StrOutputParser()\n",
    "\n",
    "name_collection_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are helpful assistant writing a topic name to assign this question. \n",
    "    Please provide a helpful topic name for this question and similar questions. Generally, it can be an IBM product name.\n",
    "    Only respond with the topic name with no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Topic Name: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "name_collection_chain = name_collection_prompt | watsonx_llama3 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc16d8f",
   "metadata": {},
   "source": [
    "## Discovery Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8833cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watson Discovery Utils\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "    \"\"\"description\"\"\"\n",
    "    # Remove HTML tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    cleaned_text = re.sub(clean, '', text)\n",
    "    \n",
    "    # Replace multiple newline characters with a single newline\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def watson_discovery_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    A tool to query IBM Watson Discovery for insights from unstructured data.\n",
    "    The input query should be a natural language query.\n",
    "    \"\"\"\n",
    "    discovery = DiscoveryV2(version=\"2019-11-29\", authenticator=authenticator)\n",
    "    discovery.set_service_url(os.getenv(\"DISCOVERY_SERVICE_URL\"))\n",
    "\n",
    "    response = discovery.query(project_id=discovery_project_id, natural_language_query=query).get_result()\n",
    "    results = response[\"results\"]\n",
    "    passages = [x[\"document_passages\"] for x in results]\n",
    "    excerpts = [x[\"passage_text\"]  for l in passages for x in l]\n",
    "    context = \"\\n\\n\".join(excerpts)\n",
    "\n",
    "    return clean_text(context)\n",
    "\n",
    "def watson_discovery_summary_collections() -> str:\n",
    "    \"\"\"\n",
    "    A tool to query IBM Watson Discovery for insights from unstructured data.\n",
    "    The input query should be a natural language query.\n",
    "    \"\"\"\n",
    "\n",
    "    discovery = DiscoveryV2(version=\"2019-11-29\", authenticator=authenticator)\n",
    "    discovery.set_service_url(os.getenv(\"DISCOVERY_SERVICE_URL\"))\n",
    "\n",
    "    # List collections\n",
    "    collections = discovery.list_collections(project_id=discovery_project_id).get_result()\n",
    "    # Extract collection IDs\n",
    "    collection_ids = [collection['collection_id'] for collection in collections['collections']]\n",
    "    # Initialize an empty dictionary\n",
    "    collection_summaries = {}\n",
    "\n",
    "\n",
    "    for collection in collection_ids:\n",
    "        response = discovery.query(project_id=\"ab6d11d1-e16e-4b87-9efe-3d1c5f2ce17a\", collection_ids= [collection], natural_language_query=\"\").get_result()\n",
    "        results = response[\"results\"]\n",
    "        passages = [x[\"document_passages\"] for x in results]\n",
    "        excerpts = [x[\"passage_text\"]  for l in passages for x in l]\n",
    "        context = \"\\n\\n\".join(excerpts)\n",
    "\n",
    "        # Run chain\n",
    "        context = clean_text(context)\n",
    "        summary = summary_chain.invoke({\"context\": context})\n",
    "        collection_summaries[collection] = clean_text(summary)\n",
    "\n",
    "\n",
    "    return collection_summaries\n",
    "\n",
    "def watson_discovery_add_document(question: str, answer: str, collection_id) -> None:\n",
    "    \"\"\"\n",
    "    A function to add a document to a specific collection in IBM Watson Discovery.\n",
    "    \"\"\"\n",
    "    discovery = DiscoveryV2(version=\"2019-11-29\", authenticator=authenticator)\n",
    "    discovery.set_service_url(os.getenv(\"DISCOVERY_SERVICE_URL\"))\n",
    "\n",
    "    question = question.strip()\n",
    "    answer = answer.strip()\n",
    "    document_content = f\"USER QUESTION: {question}\\n\\nUSEFUL ANSWER:\\n{answer}\"\n",
    "    print(document_content)\n",
    "    print(\"------------\")\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
    "            temp_file.write(document_content)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "    # Add the document to Watson Discovery using the temporary file\n",
    "    with open(temp_file_path, 'rb') as file:\n",
    "        response = discovery.add_document(\n",
    "            project_id=discovery_project_id,\n",
    "            collection_id=collection_id,\n",
    "            file=file\n",
    "        ).get_result()\n",
    "\n",
    "\n",
    "    os.remove(temp_file_path)\n",
    "    \n",
    "    print(f\"Document added successfully. Document ID: {response['document_id']}\")\n",
    "    \n",
    "def watson_discovery_create_collection(question: str) -> str:\n",
    "    \"\"\"\n",
    "    A function to create a collection in IBM Watson Discovery.\n",
    "    \"\"\"\n",
    "    # Chain\n",
    "    \n",
    "    name = name_collection_chain.invoke({\"question\": question}).strip()\n",
    "    discovery = DiscoveryV2(version=\"2019-11-29\", authenticator=authenticator)\n",
    "    discovery.set_service_url(os.getenv(\"DISCOVERY_SERVICE_URL\"))\n",
    "\n",
    "    response = discovery.create_collection(project_id=discovery_project_id,\n",
    "                                name=name).get_result()\n",
    "    \n",
    "    \n",
    "    print(f\"New Collection successfully created. \\n Name: {name} \\n Collection ID: {response['collection_id']}\")\n",
    "    \n",
    "    return response['collection_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2326b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with model training, development, visual modeling, and synthetic data generation IBM watsonx.ai Foundation Models Library – available today IBM Granite Llama 3 models LAB Aligned models © © A © © © © granite\\nOther product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on ibm.com/trademark. Foundation models and Generative AI are bringing an inflection point in AI\\n| | \\\\ > F | ( S / > C | ¢ | | | | % 2 © 2023 IBM Corporation IBM's AI assistants use Foundation Models and Automation to orchestrate skill execution and reduce time and effort Customers, employees, and\\nSource links: Foundation model parameters: decoding and stopping criteria Foundation models Choosing a foundation model in watsonx. ai © © ® 14 © 2023 IBM Corporation Advantages of Retrieval Augmented Generation (RAG) RAG is a form of architecture that uses search results to enhance the generative model.\\n. | & | • Create a custom language model simply by uploading a text file with training phrases ; • Basic UI available to create STT Custom Models AM Customizations LM Customizations Docs • Available for download from Github Customiz..\\nUSER QUESTION: What foundation models can I use in watson assistant?\\nUSEFUL ANSWER:\\nYou can use the following foundation models in Watson Assistant:\\n1.\\nUSER QUESTION: What foundation models can I use in watson assistant?\\nUSEFUL ANSWER:\\nYou can use the following foundation models in Watson Assistant:\\n1.\\nbusiness users, data engineers, and data scientists) Ensure accountability and auditability of ML models Quantifiable measure of business risk exposure from deployed ML models Client example Challenge Solution\\nOther product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on ibm.com/trademark. Produced in the United States of America June 2023 This document is current as of the initial date of publication and may be changed by IBM at any time.\\nLearn more We see a future where data fabric systems will simplify data management in hybrid and multicloud landscapes— transforming governance, compliance and integration. That future is available now; read this white paper to see how data fabric concept is already helping companies in many industries.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watson_discovery_tool(\"what foundation models are available?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9048a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e7b5cf24-2148-7ed5-0000-018f54532efc': 'IBM Watsonx is an AI and data platform that helps deploy and embed AI across businesses, with high-impact use cases like chatbots. Watson Assistant is a conversational AI application that can be used to quickly deploy chatbots. In Watson Assistant, you can use foundation models like IBM Granite Code LLM and Code Llama.', '6042175d-e2c0-b031-0000-018f5451ff1d': 'Here is a 3-sentence summary:\\nIBM Watsonx.data is an open, hybrid, and governed data store that helps scale AI workloads by providing a trusted data foundation. It offers fit-for-purpose query engines, built-in data governance, security, and automation. A data fabric architecture can simplify data access for users, addressing the challenges of dispersed, dynamic, and diverse data.', '583b032f-e9f6-8596-0000-018f5452b6b3': \"Watsonx Orchestrate is an intelligent assistant that boosts productivity by efficiently resolving tasks across platforms. It's part of IBM's strategy to implement AI assistants in various business areas, including customer care, sales, supply chain, procurement, finance, and HR. A new feature, Conversational Skills, is set to be released in April 2024.\"}\n"
     ]
    }
   ],
   "source": [
    "collection_summaries = watson_discovery_summary_collections()\n",
    "print(collection_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e656f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watson_discovery_create_collection(\"what is IBM Knowledge Catalog?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf93d5",
   "metadata": {},
   "source": [
    "## Helper Prompts for QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "retrieval_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains any information useful to answer user's question, \n",
    "    grade it as relevant. If the document contains information that is relevant to only a portion of the question, grade it as relevant.\n",
    "    The goal is to filter out completely irrelevant retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = retrieval_grader_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "question = \"What are Watson Orchestrate skills?\"\n",
    "docs = watson_discovery_tool(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": docs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1711c67",
   "metadata": {},
   "source": [
    "The above should be yes -- needs fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1fd779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Watson Orchestrate skills list'}, {'query': 'Watson Orchestrate features'}, {'query': 'IBM Watson Orchestrate capabilities'}, {'query': 'Watson Orchestrate AI skills'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "reword_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an in charge of querying a search engine on behalf of a user.\n",
    "    A user will provide you a question that you need to reword for optimal search on business documents. If there are multiple questions, break them down into multiple queries.\n",
    "    For example, if a user asks to compare the forecast in Chicago to New York, you should form queries such as \"Weather forecast NYC\", \"Weather forecast Chicago\", \"current weather NYC\", \"current weather Chicago\", etc.\n",
    "    Provide up to 4 queries in a JSON list with a single key 'query' for each query and no premable or explaination.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "reword_chain = reword_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are Watson Orchestrate skills?\"\n",
    "generation = reword_chain.invoke({\"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "693f48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the skills of Watson Orchestrate Product Manager?'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "reword_feedback_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an in charge of querying a search engine on behalf of a user.\n",
    "    Analyze the current question and corresponding document. The current query is not optimal and needs to be reworded for better results. \n",
    "    Provide the query as a JSON with a single key 'query' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Query: {query} \n",
    "    Document: {document} \n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\", \"document\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "reword_feedback_chain = reword_feedback_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "\n",
    "# Run\n",
    "\n",
    "question = \"What are Watson Orchestrate skills?\"\n",
    "docs = watson_discovery_tool(question)\n",
    "print(reword_feedback_chain.invoke({\"query\": question, \"document\": docs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Watson Orchestrate skills are AI-powered skills that leverage Gen AI to accomplish new tasks or build new skills, and execute automations. They are part of the IBM Watson Orchestrate platform, which helps companies empower and scale their workforce by democratizing automations through digital workers and assistants.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Answer the question completely while keeping the answer concise. Don't include extraneous information <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = rag_prompt | watsonx_llama3 | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are Watson Orchestrate skills?\"\n",
    "docs = watson_discovery_tool(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Prompt\n",
    "hallucination_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_grader_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# Prompt\n",
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = answer_grader_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3beb5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query_router_prompt(collection_summaries):\n",
    "\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an expert at routing a query to the appropriate data source.\n",
    "    Use the following criteria to determine the data source:\\n\"\"\"\n",
    "    for cid, summary in collection_summaries.items():\n",
    "        template = template + f\"\\t- {cid}: {summary}\\n\"\n",
    "\n",
    "    template = template + \"\"\"\\t- web_search: If the query does not match any of the collections above.\n",
    "\n",
    "    Provide your decision as a JSON object with a single key 'datasource' and no preamble or explanation.\n",
    "    Query to route: {query} \n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "    route_query_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    )\n",
    "    \n",
    "    # print(template)\n",
    "\n",
    "    # Chain\n",
    "    # route_query_chain = route_query_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "    \n",
    "    return route_query_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3e9ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datasource': '583b032f-e9f6-8596-0000-018f5452b6b3'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_query_prompt = construct_query_router_prompt(collection_summaries)\n",
    "route_query_chain = route_query_prompt | watsonx_llama3 | JsonOutputParser()\n",
    "\n",
    "query = \"What are Watson Orchestrate skills?\"\n",
    "route_query_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd59cdf-a04d-4b2e-b9cc-6a1b1e80a6c6",
   "metadata": {},
   "source": [
    "## Implement chains above as a control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_document(query, document, datasource):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved document are relevant to the query\n",
    "    If the document is not relevant, we will run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    #for query, document, datasource in zip(state['queries'], documents, datasources):\n",
    "    result = retrieval_grader.invoke({\"question\": query, \"document\": document.page_content})\n",
    "    grade = result['score']\n",
    "    # Document relevant\n",
    "    if grade.lower() == \"yes\":\n",
    "        print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "        return document\n",
    "    # Document not relevant\n",
    "    else:\n",
    "        print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        # Invoke web search to replace irrelevant document\n",
    "        if datasource != \"web_search\":\n",
    "            document = web_search(query)\n",
    "            result = retrieval_grader.invoke({\"question\": query, \"document\": document.page_content})\n",
    "            grade = result['score']\n",
    "       # Generate new web results to replace irrelevant web search\n",
    "        i = 0\n",
    "        while grade.lower() != \"yes\" and i < 3:\n",
    "            print(\"---REWORDING WEB QUERY---\")\n",
    "\n",
    "            query = reword_feedback_chain.invoke({\"query\": query, \"document\": document.page_content})\n",
    "            document = web_search(query['query'])\n",
    "            result = retrieval_grader.invoke({\"question\": query, \"document\": document.page_content})\n",
    "            grade = result['score']\n",
    "\n",
    "            i+= 1\n",
    "        return document\n",
    "\n",
    "    \n",
    "    \n",
    "def web_search(query):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    # Web search\n",
    "    str_docs = web_search_tool.invoke({\"query\": query})\n",
    "    docs = parse_ddg_results(str_docs)\n",
    "    web_results = \"\\n\".join([d[\"snippet\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    \n",
    "    documents = web_results\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4021f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_analyzer(state):\n",
    "    \"\"\"\n",
    "    Break down the question into multiple queries for search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with generated queries\n",
    "    \"\"\"\n",
    "    print(\"---QUERY ANALYZER---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Generate queries using the reword_chain\n",
    "    queries = reword_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Append original question as a query as well\n",
    "    queries.append({\"query\": question})\n",
    "    \n",
    "    return {\"question\": question, \"queries\": queries}\n",
    "\n",
    "def route_query(query):\n",
    "    \"\"\"\n",
    "    Route query to Watson Discovery or web search using an LLM.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        query (str): The query to route\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ROUTE QUERY---\")\n",
    "    \n",
    "    # Use the LLM to determine the data source\n",
    "    result = route_query_chain.invoke({\"query\": query})\n",
    "    datasource = result['datasource']\n",
    "    \n",
    "    return datasource\n",
    "\n",
    "\n",
    "\n",
    "def watson_discovery_retrieve(query, collection):\n",
    "    \"\"\"\n",
    "    Retrieve documents from Watson Discovery based on the collection.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        collection (str): The selected collection\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"---WATSON DISCOVERY RETRIEVE ({collection})---\")\n",
    "    \n",
    "    # Retrieve documents from Watson Discovery\n",
    "    result = watson_discovery_tool(query)\n",
    "    \n",
    "    disc_results = Document(page_content=result)\n",
    "    \n",
    "    documents = disc_results\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "    \n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95950868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_question, query, datasource):\n",
    "    print(f\"Processing query: {query}\")\n",
    "    \n",
    "    # Route the query\n",
    "    print(\"Datasource: \" + datasource)\n",
    "    \n",
    "    if datasource == \"web_search\":\n",
    "        documents = web_search(query)\n",
    "    else:\n",
    "        try:\n",
    "            summary = collection_summaries[datasource]\n",
    "        except:\n",
    "            print(\"Router returned a collection ID not in dictionary\")\n",
    "            return []\n",
    "        documents = watson_discovery_retrieve(query, datasource)\n",
    "        \n",
    "    \n",
    "    documents = grade_document(query, documents, datasource)\n",
    "    return documents\n",
    "\n",
    "def run_pipeline(question):\n",
    "    # Get the initial question\n",
    "    state = {\"question\": question}\n",
    "    \n",
    "    # Run the query analyzer\n",
    "    state = query_analyzer(state)\n",
    "    queries = state[\"queries\"]\n",
    "\n",
    "    state[\"datasources\"] = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(route_query)(query['query']) for query in queries)\n",
    "    datasources = state[\"datasources\"]\n",
    "    \n",
    "    # Process each query in parallel using joblib\n",
    "    documents = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(process_query)(state[\"question\"], query['query'], datasource)\n",
    "                                                      for query, datasource in zip(queries, datasources))\n",
    "    \n",
    "    state[\"documents\"] = documents\n",
    "    \n",
    "    #state = grade_documents(state)\n",
    "    state = generate(state)\n",
    "    state[\"answer_class\"] = grade_generation_v_documents_and_question(state)\n",
    "    # Print the final results\n",
    "    print(\"Final Answer:\\n\" + state[\"generation\"])\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e8539cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---QUERY ANALYZER---\n",
      "---ROUTE QUERY---\n",
      "---ROUTE QUERY---\n",
      "---ROUTE QUERY---\n",
      "---ROUTE QUERY---\n",
      "---ROUTE QUERY---\n",
      "Processing query: Watson Assistant foundation modelsProcessing query: Watson Assistant AI models comparison\n",
      "Datasource: e7b5cf24-2148-7ed5-0000-018f54532efc\n",
      "---WATSON DISCOVERY RETRIEVE (e7b5cf24-2148-7ed5-0000-018f54532efc)---\n",
      "\n",
      "Datasource: e7b5cf24-2148-7ed5-0000-018f54532efc\n",
      "---WATSON DISCOVERY RETRIEVE (e7b5cf24-2148-7ed5-0000-018f54532efc)---\n",
      "Processing query: OpenAI models vs Watson Assistant performance\n",
      "Datasource: e7b5cf24-2148-7ed5-0000-018f54532efc\n",
      "---WATSON DISCOVERY RETRIEVE (e7b5cf24-2148-7ed5-0000-018f54532efc)---\n",
      "Processing query: Foundation models in Watson Assistant vs OpenAI\n",
      "Datasource: e7b5cf24-2148-7ed5-0000-018f54532efc\n",
      "---WATSON DISCOVERY RETRIEVE (e7b5cf24-2148-7ed5-0000-018f54532efc)---\n",
      "Processing query: What foundation models can I use in watson assistant and how do they compare performatively to OpenAI?\n",
      "Datasource: e7b5cf24-2148-7ed5-0000-018f54532efc\n",
      "---WATSON DISCOVERY RETRIEVE (e7b5cf24-2148-7ed5-0000-018f54532efc)---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---WEB SEARCH---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---WEB SEARCH---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Final Answer:\n",
      "\n",
      "\n",
      "You can use various foundation models in Watson Assistant, including IBM-developed, open-source, third-party, and custom models. These models support different tasks such as conversational search, retrieval augmented generation (RAG), and more.\n",
      "\n",
      "As for the performance comparison with OpenAI, it's difficult to provide a direct comparison without specific metrics or benchmarks. However, it's worth noting that Watson Assistant's foundation models are designed to provide high accuracy and performance, with some models achieving similar accuracy to OpenAI models with significantly less training data.\n",
      "\n",
      "Additionally, Watson Assistant's integration with watsonx.ai provides access to a range of foundation models and tools for customization, tuning, and deployment, which can enhance performance and flexibility. OpenAI's models, on the other hand, are available through their APIs, such as the Assistants API, which provides access to models like GPT-4o.\n",
      "\n",
      "In terms of specific performance metrics, one user reported slow performance with the Assistants API, with response times ranging\n",
      "Time to execute: 29.579232215881348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "# question = \"How does watson orchestrate differ from watson assistant?\"\n",
    "# question = \"How does IBM Knowledge Catalog compare to Informatica?\"\n",
    "question = \"What foundation models can I use in watson assistant and how do they compare performatively to OpenAI?\"\n",
    "result = run_pipeline(question)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time to execute: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "354915ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Collection successfully created. \n",
      " Name: IBM Cloud Pak for Data \n",
      " Collection ID: d34c4f1e-dd5b-2737-0000-018f976cd037\n",
      "USER QUESTION: How does IBM Knowledge Catalog compare to Informatica?\n",
      "\n",
      "USEFUL ANSWER:\n",
      "IBM Knowledge Catalog and Informatica Enterprise Data Catalog are both data cataloging solutions that provide features for data governance, data quality, and data discovery. They share some similarities, such as offering data lineage tracking and data quality scores. However, they also have some differences, such as IBM Knowledge Catalog's focus on AI-assisted self-service discovery and its integration with Cloud Pak for Data as a Service, whereas Informatica Enterprise Data Catalog is powered by a machine-learning-based discovery engine.\n",
      "------------\n",
      "Document added successfully. Document ID: f3864e65-a3ca-4797-9a24-8cbd99e3e6bb\n"
     ]
    }
   ],
   "source": [
    "if result[\"answer_class\"] == \"useful\":\n",
    "    collection_id = route_query_chain.invoke({\"query\": question})[\"datasource\"]\n",
    "    \n",
    "    if collection_id == \"web_search\": # There is no relevant collection so let's create one\n",
    "        collection_id = watson_discovery_create_collection(question)\n",
    "        summary = summary_chain.invoke({\"context\": result[\"generation\"]})\n",
    "        collection_summaries[collection_id] = summary\n",
    "        \n",
    "    watson_discovery_add_document(question, result[\"generation\"], collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc88ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflect",
   "language": "python",
   "name": "reflect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b1234d2fdb3d81c7c62eb5cb9d4f98d639f6682094e7fc7ea2d35604c5ac668"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
